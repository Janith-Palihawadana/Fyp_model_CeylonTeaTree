{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZCd74JzEjie",
        "outputId": "cbaceed7-5319-4aa4-dfbb-66108620b4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKSBzUzHEmZi",
        "outputId": "6f69308c-9943-4b0d-b6d3-2155fcfa9362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EVUbYH9i8Eh",
        "outputId": "c4c43c5c-33be-4bec-df02-a115b99ff605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "BJwuwdCqi-_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Sz2t85objCbr",
        "outputId": "8d6e89cb-f24a-4581-9962-c7eef6db7061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Flatten ,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import ResNet152V2\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random"
      ],
      "metadata": {
        "id": "InuBTtpbjGqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/FYP/Data_set/train\n",
        "\n",
        "!mkdir train\n",
        "!mkdir test\n",
        "\n",
        "train_percent = 0.8\n",
        "test_percent = 0.1\n",
        "val_percent = 0.1\n",
        "\n",
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "classes = [\"algal leaf\", \"Anthracnose\", \"bird eye spot\", \"brown blight\", \"gray light\", \"healthy\", \"red leaf spot\", \"white spot\"]\n",
        "\n",
        "for c in classes:\n",
        "    os.makedirs(\"train/\" + c)\n",
        "    os.makedirs(\"test/\" + c)\n",
        "    os.makedirs(\"val/\" + c)\n",
        "    files = os.listdir(c)\n",
        "    num_train = int(len(files) * train_percent)\n",
        "    num_test = int(len(files) * test_percent)\n",
        "    num_val = int(len(files) * val_percent)\n",
        "    train_files = random.sample(files, num_train)\n",
        "    test_val_files = list(set(files) - set(train_files))\n",
        "    test_files = random.sample(test_val_files, num_test)\n",
        "    val_files = list(set(test_val_files) - set(test_files))\n",
        "    for f in train_files:\n",
        "        copyfile(c + \"/\" + f, \"train/\" + c + \"/\" + f)\n",
        "    for f in test_files:\n",
        "        copyfile(c + \"/\" + f, \"test/\" + c + \"/\" + f)\n",
        "    for f in val_files:\n",
        "        copyfile(c + \"/\" + f, \"val/\" + c + \"/\" + f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uunJwhJEjIRp",
        "outputId": "c3558d87-8a42-4d68-ce18-4c2cac68bc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FYP/Data_set/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize images\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "image_size = [img_height, img_width, 3] # height, width, depth\n",
        "\n",
        "# Output classes\n",
        "predict_class_size = 8\n",
        "\n",
        "# batch size\n",
        "bs = 32\n",
        "\n",
        "# Train/test image path\n",
        "train_data = '/content/drive/My Drive/FYP/Data_set/train/'\n",
        "test_data = '/content/drive/My Drive/FYP/Data_set/val/'"
      ],
      "metadata": {
        "id": "ZqpMq4R1jOS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = {ResNet152V2:\"resnet152v2\"}"
      ],
      "metadata": {
        "id": "FJ0UuEP--Qd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_acc(model_history, model_name):\n",
        "    \n",
        "    ## Plot loss\n",
        "    plt.plot(model_history.history['loss'], label='train loss')\n",
        "    plt.plot(model_history.history['val_loss'], label='test loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('/content/drive/My Drive/FYP/Data_set/model8/loss_{}'.format(model_name))\n",
        "    plt.clf()\n",
        "\n",
        "    ## Plot Accuracy\n",
        "    plt.plot(model_history.history['accuracy'], label='train accuracy')\n",
        "    plt.plot(model_history.history['val_accuracy'], label='test accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig('/content/drive/My Drive/FYP/Data_set/model8/accuracy_{}'.format(model_name))\n",
        "    plt.clf()"
      ],
      "metadata": {
        "id": "BnQJXaNtjrx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def train_model(pre_trained_model):\n",
        "    \n",
        "    # Intialize pre trained model (ResNet152V2) with imagenet weights\n",
        "    pre_trained_net = pre_trained_model(input_shape=image_size, weights='imagenet', include_top=False)\n",
        "\n",
        "    # Unfreeze some of the layers for fine-tuning\n",
        "    for layer in pre_trained_net.layers[-15:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Add new layers on top of the pre-trained model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(224,224,3)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
        "\n",
        "    model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
        "\n",
        "    model.add(Conv2D(64, (3,3), 1, activation='relu'))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
        "    model.add(Dense(8, activation='softmax'))    \n",
        "\n",
        "    # Set cost and optimization functions for model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Image Data Augmentation using keras ImageDataGenerator\n",
        "    # train data generator\n",
        "    train_aug = ImageDataGenerator(rescale=1/255, \n",
        "                                   rotation_range=10,                         \n",
        "                                   zoom_range=0.2, # ~ [1-0.2, 1+0.2]\n",
        "                                   brightness_range=[0.5,1.5]) # <1 darkens img, >1 brightens img\n",
        "    # test data generator\n",
        "    test_aug = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "    # Create train/test set\n",
        "    train_set = train_aug.flow_from_directory(train_data, \n",
        "                                              target_size=(img_height,img_width), \n",
        "                                              batch_size=bs, \n",
        "                                              class_mode='categorical')\n",
        "    \n",
        "\n",
        "    test_set = test_aug.flow_from_directory(test_data, \n",
        "                                            target_size=(img_height,img_width), \n",
        "                                            batch_size=bs, \n",
        "                                            class_mode='categorical')   \n",
        "\n",
        "    # fit the model\n",
        "    hist = model.fit_generator(train_set, \n",
        "                              epochs=50,\n",
        "                              steps_per_epoch=len(train_set),\n",
        "                              validation_data=test_set,\n",
        "                              validation_steps=len(test_set)\n",
        "                             )\n",
        "    \n",
        "    # model name\n",
        "    model_name = model_dict[pre_trained_model]\n",
        "    \n",
        "    # Save model loss/accuracy plots\n",
        "    plot_loss_acc(hist,model_name)\n",
        "    \n",
        "    # Save model\n",
        "    model.save('/content/drive/My Drive/FYP/Data_set/model8/model_{}.h5'.format(model_name))\n",
        "    \n",
        "    print(\"model {} trained and saved\".format(model_name))"
      ],
      "metadata": {
        "id": "PCTgOv0Xmkhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for each_model in list(model_dict.keys()):\n",
        "    train_model(each_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HvXAe2MSms5o",
        "outputId": "77113f95-607e-4b30-ee88-cd738e8f0240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234545216/234545216 [==============================] - 9s 0us/step\n",
            "Found 776 images belonging to 8 classes.\n",
            "Found 80 images belonging to 8 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-878c26066087>:60: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  hist = model.fit_generator(train_set,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 693s 28s/step - loss: 2.4679 - accuracy: 0.1379 - val_loss: 2.0509 - val_accuracy: 0.3500\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 158s 6s/step - loss: 1.8445 - accuracy: 0.2719 - val_loss: 1.7595 - val_accuracy: 0.2625\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 1.5408 - accuracy: 0.3454 - val_loss: 1.6152 - val_accuracy: 0.2875\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 157s 6s/step - loss: 1.2895 - accuracy: 0.3943 - val_loss: 1.3131 - val_accuracy: 0.3750\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 1.1393 - accuracy: 0.4832 - val_loss: 1.3346 - val_accuracy: 0.3750\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 1.1095 - accuracy: 0.5219 - val_loss: 1.1126 - val_accuracy: 0.4625\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 149s 6s/step - loss: 1.0357 - accuracy: 0.5258 - val_loss: 1.0842 - val_accuracy: 0.6000\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 156s 6s/step - loss: 1.0128 - accuracy: 0.5722 - val_loss: 1.1036 - val_accuracy: 0.5375\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.9264 - accuracy: 0.6044 - val_loss: 1.0270 - val_accuracy: 0.6250\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 160s 6s/step - loss: 0.8344 - accuracy: 0.6534 - val_loss: 0.9183 - val_accuracy: 0.6625\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 156s 6s/step - loss: 0.8691 - accuracy: 0.6534 - val_loss: 0.9884 - val_accuracy: 0.6500\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.8202 - accuracy: 0.6637 - val_loss: 0.8062 - val_accuracy: 0.7000\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.8894 - accuracy: 0.6224 - val_loss: 0.8741 - val_accuracy: 0.7375\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 152s 6s/step - loss: 0.7679 - accuracy: 0.6894 - val_loss: 0.7054 - val_accuracy: 0.7875\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.7021 - accuracy: 0.6869 - val_loss: 0.8354 - val_accuracy: 0.7375\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.7050 - accuracy: 0.7113 - val_loss: 0.8403 - val_accuracy: 0.7250\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 160s 6s/step - loss: 0.6836 - accuracy: 0.7178 - val_loss: 0.7977 - val_accuracy: 0.7125\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.7243 - accuracy: 0.7113 - val_loss: 0.7227 - val_accuracy: 0.7250\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 148s 6s/step - loss: 0.6585 - accuracy: 0.7371 - val_loss: 0.8008 - val_accuracy: 0.7500\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.6218 - accuracy: 0.7423 - val_loss: 0.6142 - val_accuracy: 0.8250\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.6197 - accuracy: 0.7513 - val_loss: 0.8960 - val_accuracy: 0.7250\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.5900 - accuracy: 0.7590 - val_loss: 0.8437 - val_accuracy: 0.7250\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 155s 6s/step - loss: 0.6070 - accuracy: 0.7719 - val_loss: 0.9018 - val_accuracy: 0.6750\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 155s 6s/step - loss: 0.5530 - accuracy: 0.7771 - val_loss: 0.7020 - val_accuracy: 0.7750\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 0.5174 - accuracy: 0.8028 - val_loss: 0.5743 - val_accuracy: 0.7875\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.5839 - accuracy: 0.7706 - val_loss: 0.6469 - val_accuracy: 0.7750\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.5799 - accuracy: 0.7526 - val_loss: 0.8240 - val_accuracy: 0.7250\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 0.5302 - accuracy: 0.7771 - val_loss: 0.6915 - val_accuracy: 0.8125\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 156s 6s/step - loss: 0.5872 - accuracy: 0.7526 - val_loss: 0.9190 - val_accuracy: 0.7500\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 148s 6s/step - loss: 0.5197 - accuracy: 0.7951 - val_loss: 0.7677 - val_accuracy: 0.8250\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 0.4540 - accuracy: 0.8209 - val_loss: 0.5920 - val_accuracy: 0.8375\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.4730 - accuracy: 0.8067 - val_loss: 0.7212 - val_accuracy: 0.8250\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 148s 6s/step - loss: 0.4535 - accuracy: 0.8235 - val_loss: 0.6742 - val_accuracy: 0.8375\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.4792 - accuracy: 0.8015 - val_loss: 0.5274 - val_accuracy: 0.8875\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.4542 - accuracy: 0.8325 - val_loss: 0.5981 - val_accuracy: 0.8250\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.4317 - accuracy: 0.8299 - val_loss: 0.7647 - val_accuracy: 0.8250\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 155s 6s/step - loss: 0.4317 - accuracy: 0.8286 - val_loss: 0.6881 - val_accuracy: 0.8750\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.3941 - accuracy: 0.8544 - val_loss: 0.7380 - val_accuracy: 0.7875\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.4262 - accuracy: 0.8338 - val_loss: 0.8175 - val_accuracy: 0.7875\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.3743 - accuracy: 0.8492 - val_loss: 0.6861 - val_accuracy: 0.8000\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.3676 - accuracy: 0.8595 - val_loss: 0.7956 - val_accuracy: 0.7750\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 155s 6s/step - loss: 0.3892 - accuracy: 0.8531 - val_loss: 0.7018 - val_accuracy: 0.8375\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.4275 - accuracy: 0.8157 - val_loss: 0.6560 - val_accuracy: 0.8625\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.4785 - accuracy: 0.8286 - val_loss: 0.6546 - val_accuracy: 0.8500\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 0.4660 - accuracy: 0.8144 - val_loss: 0.7677 - val_accuracy: 0.8375\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 147s 6s/step - loss: 0.3572 - accuracy: 0.8582 - val_loss: 0.7505 - val_accuracy: 0.8500\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.3884 - accuracy: 0.8582 - val_loss: 0.9201 - val_accuracy: 0.7750\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 145s 6s/step - loss: 0.3728 - accuracy: 0.8531 - val_loss: 0.7551 - val_accuracy: 0.8750\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 146s 6s/step - loss: 0.4126 - accuracy: 0.8312 - val_loss: 0.5833 - val_accuracy: 0.8250\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 157s 6s/step - loss: 0.3809 - accuracy: 0.8363 - val_loss: 0.6286 - val_accuracy: 0.8625\n",
            "model resnet152v2 trained and saved\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pb8kLxfH8a91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drVoc7cK8kwi",
        "outputId": "e06268c0-e0ec-4a27-886e-9d6ab5c4383c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/drive/My Drive/FYP/Data_set/model8/model_resnet152v2.h5')"
      ],
      "metadata": {
        "id": "T1nd7fhd8pgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TejjITgI8-9S",
        "outputId": "1ba9d216-ff89-487f-9b98-0effd06e6414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 111, 111, 16)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 111, 111, 16)      0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 54, 54, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 54, 54, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 26, 26, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 26, 26, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 43264)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               11075840  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 2056      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,101,480\n",
            "Trainable params: 11,101,480\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img"
      ],
      "metadata": {
        "id": "HehubroA9vow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_aug = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_aug = ImageDataGenerator(rescale=1/255, \n",
        "                                   rotation_range=10,                         \n",
        "                                   zoom_range=0.2, # ~ [1-0.2, 1+0.2]\n",
        "                                   brightness_range=[0.5,1.5]) # <1 darkens img, >1 brightens img"
      ],
      "metadata": {
        "id": "a2F1ntKUBBVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = '/content/drive/My Drive/FYP/Data_set/val/'\n",
        "train_data = '/content/drive/My Drive/FYP/Data_set/train/'"
      ],
      "metadata": {
        "id": "ARkO-HULBLjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_height = 224\n",
        "img_width = 224\n",
        "image_size = [img_height, img_width, 3] # height, width, depth\n",
        "# batch size\n",
        "bs = 32"
      ],
      "metadata": {
        "id": "faMLlhmQCAxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_aug.flow_from_directory(test_data, \n",
        "                                            target_size=(img_height,img_width), \n",
        "                                            batch_size=bs, \n",
        "                                            class_mode='categorical')   \n",
        "\n",
        "train_set = train_aug.flow_from_directory(train_data, \n",
        "                                              target_size=(img_height,img_width), \n",
        "                                              batch_size=bs, \n",
        "                                              class_mode='categorical')\n",
        "      \n"
      ],
      "metadata": {
        "id": "DahrjXcYA_34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the classes of the test set using the trained model\n",
        "y_pred = model.predict(test_set)\n",
        "\n",
        "# Get the class labels of the test set\n",
        "y_true = test_set.classes\n",
        "\n",
        "# Get the class names\n",
        "class_names = list(train_set.class_indices.keys())\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, np.argmax(y_pred, axis=1))\n",
        "\n",
        "# Normalize the confusion matrix\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names, rotation=45)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "fmt = '.2f'\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], fmt),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.savefig('/content/drive/My Drive/FYP/Data_set/model8/matrix'.format(confusion_matrix))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iTSBi3dkAR-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/drive/My Drive/FYP/Data_set/model8/model_resnet152v2.h5')\n",
        "\n",
        "\n",
        "# Get predictions on test set\n",
        "y_pred = model.predict(test_set)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(test_set.classes, y_pred, average='weighted')\n",
        "\n",
        "print('F1 score:', f1)\n",
        "\n"
      ],
      "metadata": {
        "id": "dNpZeVipEt0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n"
      ],
      "metadata": {
        "id": "MJKDI1jhDAQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict classes for test set\n",
        "y_pred = model.predict(test_set)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_set.classes\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=list(test_set.class_indices.keys()))\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "DobEeOo7F60J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}